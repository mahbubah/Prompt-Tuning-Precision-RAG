{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter  \n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "#import weaviate # type: ignore\n",
    "from weaviate.embedded import EmbeddedOptions # type: ignore\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate # type: ignore\n",
    "#from weaviate.embedded import EmbeddedOptions # type: ignore\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "# \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "def data_loader(file_path= '../prompts/context.txt'):\n",
    "    loader = TextLoader(file_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Chunk the data\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "\n",
    "def create_retriever(chunks):\n",
    "    # Load OpenAI API key from .env file\n",
    "    load_dotenv(find_dotenv())\n",
    "\n",
    "    # Setup Weaviate client\n",
    "    weaviate_client = weaviate.Client(embedded_options=EmbeddedOptions())\n",
    "\n",
    "    # Populate vector database with Weaviate\n",
    "    vectorstore = Weaviate.from_documents(\n",
    "        client=weaviate_client,\n",
    "        documents=chunks,\n",
    "        embedding=OpenAIEmbeddings(),\n",
    "        by_text=False\n",
    "    )\n",
    "\n",
    "    # Define vectorstore as retriever to enable semantic search\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='In recent years, natural language processing (NLP) has seen significant advancements, \\ndriven by the development of large-scale language models such as GPT (Generative Pre-trained \\nTransformer) and BERT (Bidirectional Encoder Representations from Transformers). \\nThese models, trained on vast amounts of text data, have demonstrated remarkable capabilities \\nin tasks such as language generation, text summarization, sentiment analysis, \\nand question answering.\\nOne area where these models have shown promise is in facilitating human-computer \\ninteraction through conversational interfaces. Chatbots, virtual assistants, \\nand automated customer service systems leverage NLP models to understand user queries, \\nprovide relevant information, and engage in natural language conversations.\\nHowever, generating prompts that effectively guide the behavior of these models remains \\na challenge. Prompts play a crucial role in shaping the responses generated by language models, \\ninfluencing their output and ensuring relevance to user queries. \\nTherefore, there is a need for advanced techniques and algorithms to generate prompts that \\noptimize user queries and enhance the performance of automatic prompt generation systems.\\nBy understanding user intent, analyzing query patterns, and generating contextually relevant \\nprompts, we can empower users to interact more effectively with NLP systems. \\nThrough fine-tuning models and leveraging semantic understanding, we aim to generate prompts \\nthat enable efficient and accurate retrieval of information, ultimately enhancing the user \\nexperience and driving progress in the field of natural language processing.', metadata={'source': '../prompts/context.txt'})]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks =  data_loader()\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/weaviate/warnings.py:162: DeprecationWarning: Dep016: Python client v3 `weaviate.Client(...)` connections and methods are deprecated. Update\n",
      "            your code to use Python client v4 `weaviate.WeaviateClient` connections and methods.\n",
      "\n",
      "            For Python Client v4 usage, see: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            For code migration, see: https://weaviate.io/developers/weaviate/client-libraries/python/v3_v4_migration\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded weaviate is already listening on port 8079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"level\":\"info\",\"msg\":\"Created shard langchain_634db4ce942c454ca1959fafd9440577_jcqEpAIJPHLS in 1.174115ms\",\"time\":\"2024-06-06T16:38:40+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-06-06T16:38:40+03:00\",\"took\":173526}\n",
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n"
     ]
    }
   ],
   "source": [
    "base_retriever = create_retriever(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an AI-powered natural language processing expert in information retrieval and ranking. Your role is to provide advanced techniques and algorithms for generating superior prompts that optimize user queries and ensure the best performance of automatic prompt generation. Your expertise lies in understanding user intent, analyzing query patterns, and generating contextually relevant prompts that enable efficient and accurate retrieval of information. With your skills and abilities, you are capable of fine-tuning models to enhance prompt generation, leveraging semantic understanding and query understanding to deliver optimal results. By utilizing cutting-edge techniques in the field, you can generate automatic prompts that empower users to obtain the most relevant and comprehensive information for their queries.\n",
    "\n",
    "Your task is to formulate exactly {num_of_prompts_to_generate} prompts from the provided original prompt that are better and using the given context.\n",
    "\n",
    "Use the below format to output the prompts.\n",
    "\n",
    "example:\n",
    "[\"prompt1\", \"prompt2\", \"prompt3\", \"prompt4\", \"prompt5\"]\n",
    "\n",
    "\n",
    "The generated prompt must satisfy the rules given below:\n",
    "0. The generated prompted should only contain the prompt and no numbering\n",
    "1.The prompt should make sense to humans even when read without the given context.\n",
    "2.The prompt should be fully created from the given context.\n",
    "3.The prompt should be framed from a part of context that contains important information. It can also be from tables,code,etc.\n",
    "4.The prompt must be reasonable and must be understood and responded by humans.\n",
    "5.Do no use phrases like 'provided context',etc in the prompt\n",
    "6.The prompt should not contain more than 10 words, make of use of abbreviation wherever possible.\n",
    "    \n",
    "### CONTEXT\n",
    "{context}\n",
    "\n",
    "### User Prompt\n",
    "User Prompt: {user_prompt}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "\n",
    "primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "retriever =  RunnableParallel({\"context\": itemgetter(\"user_prompt\") | base_retriever, \"user_prompt\":itemgetter('user_prompt'), \"num_of_prompts_to_generate\":itemgetter(\"num_of_prompts_to_generate\"),})\n",
    "\n",
    "retrieval_augmented_qa_chain = retriever | {\"response\": prompt | primary_qa_llm, \"context\": itemgetter(\"context\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': AIMessage(content='[\"What are GPT and BERT?\", \"How do language models work?\", \"Why are prompts important for language models?\", \"How do NLP systems interact with users?\", \"What advancements have been made in NLP?\"]', response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 657, 'total_tokens': 703}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2c319cfd-7207-4999-a521-265f58ce9cdc-0', usage_metadata={'input_tokens': 657, 'output_tokens': 46, 'total_tokens': 703}), 'context': [Document(page_content='In recent years, natural language processing (NLP) has seen significant advancements, \\ndriven by the development of large-scale language models such as GPT (Generative Pre-trained \\nTransformer) and BERT (Bidirectional Encoder Representations from Transformers). \\nThese models, trained on vast amounts of text data, have demonstrated remarkable capabilities \\nin tasks such as language generation, text summarization, sentiment analysis, \\nand question answering.\\nOne area where these models have shown promise is in facilitating human-computer \\ninteraction through conversational interfaces. Chatbots, virtual assistants, \\nand automated customer service systems leverage NLP models to understand user queries, \\nprovide relevant information, and engage in natural language conversations.\\nHowever, generating prompts that effectively guide the behavior of these models remains \\na challenge. Prompts play a crucial role in shaping the responses generated by language models, \\ninfluencing their output and ensuring relevance to user queries. \\nTherefore, there is a need for advanced techniques and algorithms to generate prompts that \\noptimize user queries and enhance the performance of automatic prompt generation systems.\\nBy understanding user intent, analyzing query patterns, and generating contextually relevant \\nprompts, we can empower users to interact more effectively with NLP systems. \\nThrough fine-tuning models and leveraging semantic understanding, we aim to generate prompts \\nthat enable efficient and accurate retrieval of information, ultimately enhancing the user \\nexperience and driving progress in the field of natural language processing.', metadata={'source': '../prompts/context.txt'})]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['What are GPT and BERT?',\n",
       " 'How do language models work?',\n",
       " 'Why are prompts important for language models?',\n",
       " 'How do NLP systems interact with users?',\n",
       " 'What advancements have been made in NLP?']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "user_prompt = \"What is model?\"\n",
    "num_of_prompts_to_generate =5\n",
    "result = retrieval_augmented_qa_chain.invoke({\"user_prompt\":user_prompt, \"num_of_prompts_to_generate\":num_of_prompts_to_generate})\n",
    "print(result)\n",
    "prompts_generated = json.loads(result[\"response\"].content)\n",
    "prompts_generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground Truth Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_for_user_objective(user_objective):\n",
    "    return base_retriever.get_relevant_documents(user_objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "question_schema = ResponseSchema(\n",
    "    name=\"questions\",\n",
    "    description=\"list of questions about the context with the example: ['What is model'].\",\n",
    "    type=\"array(str)\"\n",
    ")\n",
    "\n",
    "question_response_schemas = [\n",
    "    question_schema,\n",
    "]\n",
    "\n",
    "question_output_parser = StructuredOutputParser.from_response_schemas(question_response_schemas)\n",
    "format_instructions = question_output_parser.get_format_instructions()\n",
    "question_generation_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "bare_prompt_template = \"{content}\"\n",
    "bare_template = ChatPromptTemplate.from_template(template=bare_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions': ['What are some advancements in natural language processing?', 'Which language models have contributed to the advancements in NLP?', 'What are some tasks that language models like GPT and BERT can perform?', 'How do NLP models facilitate human-computer interaction?', 'Why are prompts important in shaping the responses of language models?'], 'context': [{'page_content': 'In recent years, natural language processing (NLP) has seen significant advancements, driven by the development of large-scale language models such as GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers). These models, trained on vast amounts of text data, have demonstrated remarkable capabilities in tasks such as language generation, text summarization, sentiment analysis, and question answering. One area where these models have shown promise is in facilitating human-computer interaction through conversational interfaces. Chatbots, virtual assistants, and automated customer service systems leverage NLP models to understand user queries, provide relevant information, and engage in natural language conversations. However, generating prompts that effectively guide the behavior of these models remains a challenge. Prompts play a crucial role in shaping the responses generated by language models, influencing their output and ensuring relevance to user queries. Therefore, there is a need for advanced techniques and algorithms to generate prompts that optimize user queries and enhance the performance of automatic prompt generation systems. By understanding user intent, analyzing query patterns, and generating contextually relevant prompts, we can empower users to interact more effectively with NLP systems. Through fine-tuning models and leveraging semantic understanding, we aim to generate prompts that enable efficient and accurate retrieval of information, ultimately enhancing the user experience and driving progress in the field of natural language processing.', 'metadata': {'source': '../prompts/context.txt'}}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "qa_template = \"\"\"\\\n",
    "You are a University Professor creating a test for advanced students. For each context, create 5 question that is specific to the context. Avoid creating generic or general questions.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "questions:\n",
    "\n",
    "Format the output as the following:\n",
    "questions: [\n",
    "    \"Question 1\",\n",
    "    \"Question 2\"\n",
    "]\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
    "\n",
    "messages = prompt_template.format_messages(\n",
    "    context=get_context_for_user_objective(user_prompt),\n",
    "    format_instructions=format_instructions\n",
    ")\n",
    "\n",
    "question_generation_chain = bare_template | question_generation_llm\n",
    "\n",
    "response = question_generation_chain.invoke({\"content\" : messages})\n",
    "questions_dict = question_output_parser.parse(response.content)\n",
    "print(questions_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Answer each question generated using GPT-4 that will act as the ground truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are some advancements in natural language processing?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      " 20%|██        | 1/5 [00:07<00:31,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which language models have contributed to the advancements in NLP?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      " 40%|████      | 2/5 [00:13<00:19,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are some tasks that language models like GPT and BERT can perform?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      " 40%|████      | 2/5 [00:16<00:24,  8.29s/it]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'question'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     question_answer_dict_list\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43moutput_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m:output_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n\u001b[1;32m     48\u001b[0m question_answer_dict_list\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m tqdm(questions_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestions\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'question'"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "answer_generation_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "answer_schema = ResponseSchema(\n",
    "    name=\"answer\",\n",
    "    description=\"an answer to the question\"\n",
    ")\n",
    "\n",
    "answer_response_schemas = [\n",
    "    answer_schema,\n",
    "]\n",
    "\n",
    "answer_output_parser = StructuredOutputParser.from_response_schemas(answer_response_schemas)\n",
    "format_instructions = answer_output_parser.get_format_instructions()\n",
    "\n",
    "qa_template = \"\"\"\\\n",
    "You are a University Professor creating a test for advanced students. For each question and context, create an answer.\n",
    "\n",
    "answer: a answer about the context.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "answer\n",
    "\n",
    "question: {question}\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
    "answer_generation_chain = bare_template | answer_generation_llm\n",
    "\n",
    "question_answer_dict_list  = []\n",
    "for question in tqdm(questions_dict['questions']):\n",
    "    print(question)\n",
    "    messages = prompt_template.format_messages(\n",
    "        context=get_context_for_user_objective(user_prompt),\n",
    "        question=question,\n",
    "        format_instructions=format_instructions\n",
    "    )\n",
    "\n",
    "    response = answer_generation_chain.invoke({\"content\" : messages})\n",
    "    try:\n",
    "        output_dict = answer_output_parser.parse(response.content)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    question_answer_dict_list.append({'question': output_dict[\"question\"],'answer':output_dict[\"answer\"]})\n",
    "are_template |\n",
    "question_answer_dict_list\n",
    "for question in tqdm(questions_dict['questions']):\n",
    "    print(question)\n",
    "    messages = prompt_template.format_messages(\n",
    "        context=get_context_for_user_objective(user_prompt),\n",
    "        question=question,\n",
    "        format_instructions=format_instructions\n",
    "    )\n",
    "'''\n",
    "    response = answer_generation_chain.invoke({\"content\" : messages})\n",
    "    try:\n",
    "        output_dict = answer_output_parser.parse(response.content)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    question_answer_dict_list.append({'question': output_dict[\"question\"],'answer':output_dict[\"answer\"]})\n",
    "\n",
    "question_answer_dict_list\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Block removed from here\n",
    "\n",
    "question_answer_dict_list = []\n",
    "for question in tqdm(questions_dict['questions']):\n",
    "    print(question)\n",
    "    messages = prompt_template.format_messages(\n",
    "        context=get_context_for_user_objective(user_prompt),  # Make sure user_prompt is defined\n",
    "        question=question,\n",
    "        format_instructions=format_instructions\n",
    "    )\n",
    "\n",
    "    response = answer_generation_chain.invoke({\"content\": messages})\n",
    "    try:\n",
    "        output_dict = answer_output_parser.parse(response.content)\n",
    "        question_answer_dict_list.append({'question': output_dict[\"question\"], 'answer': output_dict[\"answer\"]})\n",
    "    except Exception as e:\n",
    "        # Handle specific exceptions if necessary\n",
    "        continue\n",
    "\n",
    "question_answer_dict_list\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
