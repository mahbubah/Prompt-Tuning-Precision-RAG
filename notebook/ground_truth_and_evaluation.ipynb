{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter  \n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "#import weaviate # type: ignore\n",
    "from weaviate.embedded import EmbeddedOptions # type: ignore\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate # type: ignore\n",
    "#from weaviate.embedded import EmbeddedOptions # type: ignore\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "# \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "def data_loader(file_path= '../prompts/context.txt'):\n",
    "    loader = TextLoader(file_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Chunk the data\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "\n",
    "def create_retriever(chunks):\n",
    "    # Load OpenAI API key from .env file\n",
    "    load_dotenv(find_dotenv())\n",
    "\n",
    "    # Setup Weaviate client\n",
    "    weaviate_client = weaviate.Client(embedded_options=EmbeddedOptions())\n",
    "\n",
    "    # Populate vector database with Weaviate\n",
    "    vectorstore = Weaviate.from_documents(\n",
    "        client=weaviate_client,\n",
    "        documents=chunks,\n",
    "        embedding=OpenAIEmbeddings(),\n",
    "        by_text=False\n",
    "    )\n",
    "\n",
    "    # Define vectorstore as retriever to enable semantic search\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='In recent years, natural language processing (NLP) has seen significant advancements, \\ndriven by the development of large-scale language models such as GPT (Generative Pre-trained \\nTransformer) and BERT (Bidirectional Encoder Representations from Transformers). \\nThese models, trained on vast amounts of text data, have demonstrated remarkable capabilities \\nin tasks such as language generation, text summarization, sentiment analysis, \\nand question answering.', metadata={'source': '../prompts/context.txt'}),\n",
       " Document(page_content='One area where these models have shown promise is in facilitating human-computer \\ninteraction through conversational interfaces. Chatbots, virtual assistants, \\nand automated customer service systems leverage NLP models to understand user queries, \\nprovide relevant information, and engage in natural language conversations.', metadata={'source': '../prompts/context.txt'}),\n",
       " Document(page_content='However, generating prompts that effectively guide the behavior of these models remains \\na challenge. Prompts play a crucial role in shaping the responses generated by language models, \\ninfluencing their output and ensuring relevance to user queries. \\nTherefore, there is a need for advanced techniques and algorithms to generate prompts that \\noptimize user queries and enhance the performance of automatic prompt generation systems.', metadata={'source': '../prompts/context.txt'}),\n",
       " Document(page_content='By understanding user intent, analyzing query patterns, and generating contextually relevant \\nprompts, we can empower users to interact more effectively with NLP systems. \\nThrough fine-tuning models and leveraging semantic understanding, we aim to generate prompts \\nthat enable efficient and accurate retrieval of information, ultimately enhancing the user \\nexperience and driving progress in the field of natural language processing.', metadata={'source': '../prompts/context.txt'}),\n",
       " Document(page_content='AI models like GPT are incredibly versatile but can sometimes generate undesirable or inappropriate responses. \\nPrompt engineering involves crafting prompts in such a way that it encourages the model to generate responses\\nthat are accurate, relevant, and contextually appropriate.', metadata={'source': '../prompts/context.txt'}),\n",
       " Document(page_content=\"Language models can generate text based on the input they receive, \\nbut they don't inherently understand context or nuance in the way humans do. \\nPrompt engineering can involve providing specific cues or constraints to guide the model \\ntowards generating outputs that align with certain criteria, \\nsuch as being factually accurate, respectful, or creative.\", metadata={'source': '../prompts/context.txt'}),\n",
       " Document(page_content='Prompt engineering can also involve fine-tuning a pre-trained language model \\non specific tasks or domains. By providing examples or prompts related to a particular task, \\nthe model can learn to generate more accurate and relevant responses for that task.\\n\\nPrompt engineering is also important from an ethical standpoint. \\nBy carefully crafting prompts, researchers and developers can help mitigate the risk of \\nAI models generating biased, harmful, or inappropriate content.', metadata={'source': '../prompts/context.txt'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks =  data_loader()\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/weaviate/warnings.py:162: DeprecationWarning: Dep016: Python client v3 `weaviate.Client(...)` connections and methods are deprecated. Update\n",
      "            your code to use Python client v4 `weaviate.WeaviateClient` connections and methods.\n",
      "\n",
      "            For Python Client v4 usage, see: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            For code migration, see: https://weaviate.io/developers/weaviate/client-libraries/python/v3_v4_migration\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded weaviate is already listening on port 8079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n"
     ]
    }
   ],
   "source": [
    "base_retriever = create_retriever(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an AI-powered natural language processing expert in information retrieval and ranking. Your role is to provide advanced techniques and algorithms for generating superior prompts that optimize user queries and ensure the best performance of automatic prompt generation. Your expertise lies in understanding user intent, analyzing query patterns, and generating contextually relevant prompts that enable efficient and accurate retrieval of information. With your skills and abilities, you are capable of fine-tuning models to enhance prompt generation, leveraging semantic understanding and query understanding to deliver optimal results. By utilizing cutting-edge techniques in the field, you can generate automatic prompts that empower users to obtain the most relevant and comprehensive information for their queries.\n",
    "\n",
    "Your task is to formulate exactly {num_of_prompts_to_generate} prompts from the provided original prompt that are better and using the given context.\n",
    "\n",
    "Use the below format to output the prompts.\n",
    "\n",
    "example:\n",
    "[\"prompt1\", \"prompt2\", \"prompt3\", \"prompt4\", \"prompt5\"]\n",
    "\n",
    "\n",
    "The generated prompt must satisfy the rules given below:\n",
    "0. The generated prompted should only contain the prompt and no numbering\n",
    "1.The prompt should make sense to humans even when read without the given context.\n",
    "2.The prompt should be fully created from the given context.\n",
    "3.The prompt should be framed from a part of context that contains important information. It can also be from tables,code,etc.\n",
    "4.The prompt must be reasonable and must be understood and responded by humans.\n",
    "5.Do no use phrases like 'provided context',etc in the prompt\n",
    "6.The prompt should not contain more than 10 words, make of use of abbreviation wherever possible.\n",
    "    \n",
    "### CONTEXT\n",
    "{context}\n",
    "\n",
    "### User Prompt\n",
    "User Prompt: {user_prompt}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "\n",
    "primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "retriever =  RunnableParallel({\"context\": itemgetter(\"user_prompt\") | base_retriever, \"user_prompt\":itemgetter('user_prompt'), \"num_of_prompts_to_generate\":itemgetter(\"num_of_prompts_to_generate\"),})\n",
    "\n",
    "retrieval_augmented_qa_chain = retriever | {\"response\": prompt | primary_qa_llm, \"context\": itemgetter(\"context\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': AIMessage(content='[\"AI models like GPT are versatile\", \"Crafting prompts for AI models\", \"Generating relevant responses with prompts\", \"Optimizing prompts for user queries\", \"Enhancing prompt generation performance\"]', response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 648, 'total_tokens': 688}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-aa873f0f-14d3-4c8d-be73-6e8c2a4f6911-0', usage_metadata={'input_tokens': 648, 'output_tokens': 40, 'total_tokens': 688}), 'context': [Document(page_content='One area where these models have shown promise is in facilitating human-computer \\ninteraction through conversational interfaces. Chatbots, virtual assistants, \\nand automated customer service systems leverage NLP models to understand user queries, \\nprovide relevant information, and engage in natural language conversations.', metadata={'source': '../prompts/context.txt'}), Document(page_content=\"Language models can generate text based on the input they receive, \\nbut they don't inherently understand context or nuance in the way humans do. \\nPrompt engineering can involve providing specific cues or constraints to guide the model \\ntowards generating outputs that align with certain criteria, \\nsuch as being factually accurate, respectful, or creative.\", metadata={'source': '../prompts/context.txt'}), Document(page_content='However, generating prompts that effectively guide the behavior of these models remains \\na challenge. Prompts play a crucial role in shaping the responses generated by language models, \\ninfluencing their output and ensuring relevance to user queries. \\nTherefore, there is a need for advanced techniques and algorithms to generate prompts that \\noptimize user queries and enhance the performance of automatic prompt generation systems.', metadata={'source': '../prompts/context.txt'}), Document(page_content='AI models like GPT are incredibly versatile but can sometimes generate undesirable or inappropriate responses. \\nPrompt engineering involves crafting prompts in such a way that it encourages the model to generate responses\\nthat are accurate, relevant, and contextually appropriate.', metadata={'source': '../prompts/context.txt'})]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['AI models like GPT are versatile',\n",
       " 'Crafting prompts for AI models',\n",
       " 'Generating relevant responses with prompts',\n",
       " 'Optimizing prompts for user queries',\n",
       " 'Enhancing prompt generation performance']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "user_prompt = \"What is model\"\n",
    "num_of_prompts_to_generate =5\n",
    "result = retrieval_augmented_qa_chain.invoke({\"user_prompt\":user_prompt, \"num_of_prompts_to_generate\":num_of_prompts_to_generate})\n",
    "print(result)\n",
    "prompts_generated = json.loads(result[\"response\"].content)\n",
    "prompts_generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground Truth Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_for_user_objective(user_objective):\n",
    "    return base_retriever.get_relevant_documents(user_objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "question_schema = ResponseSchema(\n",
    "    name=\"questions\",\n",
    "    description=\"list of questions about the context with the example: ['What is model'].\",\n",
    "    type=\"array(str)\"\n",
    ")\n",
    "\n",
    "question_response_schemas = [\n",
    "    question_schema,\n",
    "]\n",
    "\n",
    "question_output_parser = StructuredOutputParser.from_response_schemas(question_response_schemas)\n",
    "format_instructions = question_output_parser.get_format_instructions()\n",
    "question_generation_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "bare_prompt_template = \"{content}\"\n",
    "bare_template = ChatPromptTemplate.from_template(template=bare_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions': ['What are some applications of NLP models in facilitating human-computer interaction?', 'How do chatbots and virtual assistants leverage NLP models?', 'In what ways can NLP models engage in natural language conversations?', 'What is the role of prompt engineering in generating outputs aligned with certain criteria?', 'Why is prompt engineering a challenge in shaping the responses of language models?'], 'context': [{'page_content': 'One area where these models have shown promise is in facilitating human-computer interaction through conversational interfaces. Chatbots, virtual assistants, and automated customer service systems leverage NLP models to understand user queries, provide relevant information, and engage in natural language conversations.', 'metadata': {'source': '../prompts/context.txt'}}, {'page_content': \"Language models can generate text based on the input they receive, but they don't inherently understand context or nuance in the way humans do. Prompt engineering can involve providing specific cues or constraints to guide the model towards generating outputs that align with certain criteria, such as being factually accurate, respectful, or creative.\", 'metadata': {'source': '../prompts/context.txt'}}, {'page_content': 'However, generating prompts that effectively guide the behavior of these models remains a challenge. Prompts play a crucial role in shaping the responses generated by language models, influencing their output and ensuring relevance to user queries. Therefore, there is a need for advanced techniques and algorithms to generate prompts that optimize user queries and enhance the performance of automatic prompt generation systems.', 'metadata': {'source': '../prompts/context.txt'}}, {'page_content': 'AI models like GPT are incredibly versatile but can sometimes generate undesirable or inappropriate responses. Prompt engineering involves crafting prompts in such a way that it encourages the model to generate responses that are accurate, relevant, and contextually appropriate.', 'metadata': {'source': '../prompts/context.txt'}}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "qa_template = \"\"\"\\\n",
    "You are a University Professor creating a test for advanced students. For each context, create 5 question that is specific to the context. Avoid creating generic or general questions.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "questions:\n",
    "\n",
    "Format the output as the following:\n",
    "questions: [\n",
    "    \"Question 1\",\n",
    "    \"Question 2\"\n",
    "]\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
    "\n",
    "messages = prompt_template.format_messages(\n",
    "    context=get_context_for_user_objective(user_prompt),\n",
    "    format_instructions=format_instructions\n",
    ")\n",
    "\n",
    "question_generation_chain = bare_template | question_generation_llm\n",
    "\n",
    "response = question_generation_chain.invoke({\"content\" : messages})\n",
    "questions_dict = question_output_parser.parse(response.content)\n",
    "print(questions_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Answer each question generated using GPT-4 that will act as the ground truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are some applications of NLP models in facilitating human-computer interaction?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      " 20%|██        | 1/5 [00:06<00:27,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do chatbots and virtual assistants leverage NLP models?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      " 40%|████      | 2/5 [00:15<00:24,  8.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In what ways can NLP models engage in natural language conversations?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      " 60%|██████    | 3/5 [00:21<00:13,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the role of prompt engineering in generating outputs aligned with certain criteria?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      " 80%|████████  | 4/5 [00:28<00:07,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is prompt engineering a challenge in shaping the responses of language models?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      "100%|██████████| 5/5 [00:38<00:00,  7.61s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are some applications of NLP models in facilitating human-computer interaction?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do chatbots and virtual assistants leverage NLP models?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In what ways can NLP models engage in natural language conversations?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the role of prompt engineering in generating outputs aligned with certain criteria?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      " 80%|████████  | 4/5 [00:01<00:00,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is prompt engineering a challenge in shaping the responses of language models?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    response = answer_generation_chain.invoke({\"content\" : messages})\\n    try:\\n        output_dict = answer_output_parser.parse(response.content)\\n    except Exception as e:\\n        continue\\n    question_answer_dict_list.append({\\'question\\': output_dict[\"question\"],\\'answer\\':output_dict[\"answer\"]})\\n\\nquestion_answer_dict_list\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "answer_generation_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "answer_schema = ResponseSchema(\n",
    "    name=\"answer\",\n",
    "    description=\"an answer to the question\"\n",
    ")\n",
    "\n",
    "answer_response_schemas = [\n",
    "    answer_schema,\n",
    "]\n",
    "\n",
    "answer_output_parser = StructuredOutputParser.from_response_schemas(answer_response_schemas)\n",
    "format_instructions = answer_output_parser.get_format_instructions()\n",
    "\n",
    "qa_template = \"\"\"\\\n",
    "You are a University Professor creating a test for advanced students. For each question and context, create an answer.\n",
    "\n",
    "answer: a answer about the context.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "answer\n",
    "\n",
    "question: {question}\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
    "answer_generation_chain = bare_template | answer_generation_llm\n",
    "\n",
    "question_answer_dict_list  = []\n",
    "for question in tqdm(questions_dict['questions']):\n",
    "    print(question)\n",
    "    messages = prompt_template.format_messages(\n",
    "        context=get_context_for_user_objective(user_prompt),\n",
    "        question=question,\n",
    "        format_instructions=format_instructions\n",
    "    )\n",
    "\n",
    "    response = answer_generation_chain.invoke({\"content\" : messages})\n",
    "\n",
    "    try:\n",
    "        output_dict = answer_output_parser.parse(response.content)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    question_answer_dict_list.append({'question': output_dict[\"question\"],'answer':output_dict[\"answer\"]})\n",
    "\n",
    "question_answer_dict_list\n",
    "for question in tqdm(questions_dict['questions']):\n",
    "    print(question)\n",
    "    messages = prompt_template.format_messages(\n",
    "        context=get_context_for_user_objective(user_prompt),\n",
    "        question=question,\n",
    "        format_instructions=format_instructions\n",
    "    )\n",
    "'''\n",
    "    response = answer_generation_chain.invoke({\"content\" : messages})\n",
    "    try:\n",
    "        output_dict = answer_output_parser.parse(response.content)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    question_answer_dict_list.append({'question': output_dict[\"question\"],'answer':output_dict[\"answer\"]})\n",
    "\n",
    "question_answer_dict_list\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are some applications of NLP models in facilitating human-computer interaction?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      " 20%|██        | 1/5 [00:06<00:26,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do chatbots and virtual assistants leverage NLP models?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      " 40%|████      | 2/5 [00:13<00:19,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In what ways can NLP models engage in natural language conversations?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      " 60%|██████    | 3/5 [00:22<00:15,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the role of prompt engineering in generating outputs aligned with certain criteria?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      " 80%|████████  | 4/5 [00:29<00:07,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is prompt engineering a challenge in shaping the responses of language models?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/pydantic/main.py:1070: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      "100%|██████████| 5/5 [00:39<00:00,  7.81s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'question': 'What are some applications of NLP models in facilitating human-computer interaction?',\n",
       "  'answer': 'NLP models are used in facilitating human-computer interaction through conversational interfaces such as chatbots, virtual assistants, and automated customer service systems. These models help understand user queries, provide relevant information, and engage in natural language conversations.'},\n",
       " {'question': 'How do chatbots and virtual assistants leverage NLP models?',\n",
       "  'answer': 'Chatbots and virtual assistants leverage NLP models to understand user queries, provide relevant information, and engage in natural language conversations.'},\n",
       " {'question': 'In what ways can NLP models engage in natural language conversations?',\n",
       "  'answer': 'NLP models can engage in natural language conversations by understanding user queries, providing relevant information, and generating responses that align with certain criteria such as being factually accurate, respectful, or creative.'},\n",
       " {'question': 'What is the role of prompt engineering in generating outputs aligned with certain criteria?',\n",
       "  'answer': 'Prompt engineering involves providing specific cues or constraints to guide language models towards generating outputs that align with certain criteria, such as being factually accurate, respectful, or creative.'},\n",
       " {'question': 'Why is prompt engineering a challenge in shaping the responses of language models?',\n",
       "  'answer': 'Prompt engineering is a challenge in shaping the responses of language models because it involves providing specific cues or constraints to guide the model towards generating outputs that align with certain criteria, such as being factually accurate, respectful, or creative. However, generating prompts that effectively guide the behavior of these models remains a challenge, as prompts play a crucial role in shaping the responses generated by language models, influencing their output and ensuring relevance to user queries. Therefore, advanced techniques and algorithms are needed to generate prompts that optimize user queries and enhance the performance of automatic prompt generation systems.'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Block removed from here\n",
    "\n",
    "question_answer_dict_list = []\n",
    "for question in tqdm(questions_dict['questions']):\n",
    "    print(question)\n",
    "    messages = prompt_template.format_messages(\n",
    "        context=get_context_for_user_objective(user_prompt),  # Make sure user_prompt is defined\n",
    "        question=question,\n",
    "        format_instructions=format_instructions\n",
    "    )\n",
    "\n",
    "    response = answer_generation_chain.invoke({\"content\": messages})\n",
    "    try:\n",
    "        output_dict = answer_output_parser.parse(response.content)\n",
    "        question_answer_dict_list.append({'question': output_dict[\"question\"], 'answer': output_dict[\"answer\"]})\n",
    "    except Exception as e:\n",
    "        # Handle specific exceptions if necessary\n",
    "        continue\n",
    "\n",
    "question_answer_dict_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbubah/Desktop/week-7/Prompt-Tuning-Precision-RAG/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "ground_truth_qac_set = pd.DataFrame(question_answer_dict_list)\n",
    "# ground_truth_qac_set[\"context\"] = ground_truth_qac_set[\"context\"].map(lambda x: str(x.page_content))\n",
    "ground_truth_qac_set = ground_truth_qac_set.rename(columns={\"answer\" : \"ground_truth\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = Dataset.from_pandas(ground_truth_qac_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'ground_truth'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
