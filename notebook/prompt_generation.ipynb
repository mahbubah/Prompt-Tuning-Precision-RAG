{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "from openai import OpenAI\n",
    "from math import exp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\") \n",
    "vectordb_keys = os.getenv(\"VECTORDB_MODEL\") \n",
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str = vectordb_keys,\n",
    "    max_tokens=500,\n",
    "    temperature=0,\n",
    "    stop=None,\n",
    "    seed=123,\n",
    "    tools=None,\n",
    "    logprobs=None,\n",
    "    top_logprobs=None,\n",
    ") -> str:\n",
    "    \"\"\"Return the completion of the prompt.\n",
    "    @parameter messages: list of dictionaries with keys 'role' and 'content'.\n",
    "    @parameter model: the model to use for completion. Defaults to 'davinci'.\n",
    "    @parameter max_tokens: max tokens to use for each prompt completion.\n",
    "    @parameter temperature: the higher the temperature, the crazier the text\n",
    "    @parameter stop: token at which text generation is stopped\n",
    "    @parameter seed: random seed for text generation\n",
    "    @parameter tools: list of tools to use for post-processing the output.\n",
    "    @parameter logprobs: whether to return log probabilities of the output tokens or not.\n",
    "    @returns completion: the completion of the prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop\": stop,\n",
    "        \"seed\": seed,\n",
    "        \"logprobs\": logprobs,\n",
    "        \"top_logprobs\": top_logprobs,\n",
    "    }\n",
    "    if tools:\n",
    "        params[\"tools\"] = tools\n",
    "\n",
    "    completion = client.chat.completions.create(**params)\n",
    "    return completion\n",
    "\n",
    "def file_reader(path: str, ) -> str:\n",
    "    fname = os.path.join(path)\n",
    "    with open(fname, 'r') as f:\n",
    "        system_message = f.read()\n",
    "    return system_message\n",
    "\n",
    "def generate_test_data(prompt: str, context: str, num_test_output: str) -> str:\n",
    "    \"\"\"Return the classification of the hallucination.\n",
    "    @parameter prompt: the prompt to be completed.\n",
    "    @parameter user_message: the user message to be classified.\n",
    "    @parameter context: the context of the user message.\n",
    "    @returns classification: the classification of the hallucination.\n",
    "    \"\"\"\n",
    "    API_RESPONSE = get_completion(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt.replace(\"{context}\", context).replace(\"{num_test_output}\", num_test_output)\n",
    "            }\n",
    "        ],\n",
    "        model=vectordb_keys,\n",
    "        logprobs=True,\n",
    "        top_logprobs=1,\n",
    "    )\n",
    "\n",
    "    system_msg = API_RESPONSE.choices[0].message.content\n",
    "    return system_msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text data has been saved to ../prompts/automatically-generated-prompts.txt\n",
      "===========\n",
      "Prompts\n",
      "===========\n",
      "[\n",
      "\"\n",
      "Your task is to explain the advancements in natural language processing (NLP) in recent years. \n",
      "\n",
      "example:\n",
      "\n",
      "What are some of the language models that have driven advancements in NLP?\n",
      "Some of the language models that have driven advancements in NLP are GPT and BERT.\n",
      "\n",
      "The responses must satisfy the rules given below:\n",
      "1. The response should make sense to humans even when read without the given context.\n",
      "2. The response should be fully answered from the given context.\n",
      "3. Do not use phrases like 'provided context', etc in the question.\n",
      "\n",
      "context: In recent years, natural language processing (NLP) has seen significant advancements, driven by the development of large-scale language models such as GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers). \n",
      "\"\n",
      "\",\n",
      "Your task is to describe the role of prompts in shaping the responses generated by language models. \n",
      "\n",
      "example:\n",
      "\n",
      "How do prompts influence the output of language models?\n",
      "Prompts play a crucial role in shaping the responses generated by language models, influencing their output and ensuring relevance to user queries.\n",
      "\n",
      "The responses must satisfy the rules given below:\n",
      "1. The response should be fully answered from the given context.\n",
      "2. The response shouldn't be too long.\n",
      "3. The response should be framed from a part of context that contains important information. It can also be from tables, code, etc.\n",
      "\n",
      "context: Prompts play a crucial role in shaping the responses generated by language models, influencing their output and ensuring relevance to user queries. Therefore, there is a need for advanced techniques and algorithms to generate prompts that optimize user queries and enhance the performance of automatic prompt generation systems.\n",
      "\"\n",
      "\",\n",
      "Your task is to explain how NLP models facilitate human-computer interaction through conversational interfaces. \n",
      "\n",
      "example:\n",
      "\n",
      "How do NLP models facilitate human-computer interaction through conversational interfaces?\n",
      "NLP models facilitate human-computer interaction through conversational interfaces by understanding user queries, providing relevant information, and engaging in natural language conversations.\n",
      "\n",
      "The responses must satisfy the rules given below:\n",
      "1. The response should make sense to humans even when read without the given context.\n",
      "2. The response should be fully answered from the given context.\n",
      "3. Do not use phrases like 'provided context', etc in the question.\n",
      "\n",
      "context: One area where these models have shown promise is in facilitating human-computer interaction through conversational interfaces. Chatbots, virtual assistants, and automated customer service systems leverage NLP models to understand user queries\n"
     ]
    }
   ],
   "source": [
    "def main(num: str):\n",
    "    context_message = file_reader(\"../prompts/context.txt\")\n",
    "    prompt_message = file_reader(\"../prompts/prompt-generating-prompt.txt\")\n",
    "    context = str(context_message)\n",
    "    prompt = str(prompt_message)\n",
    "\n",
    "    generate_prompts = generate_test_data(prompt, context, num)\n",
    "\n",
    "    def save_txt(generate_prompts) -> None:\n",
    "        # Specify the file path\n",
    "        file_path = \"../prompts/automatically-generated-prompts.txt\"\n",
    "        with open(file_path, 'w') as txt_file:\n",
    "            txt_file.write(generate_prompts)\n",
    "        \n",
    "        print(f\"Text data has been saved to {file_path}\")\n",
    "\n",
    "    save_txt(generate_prompts)\n",
    "\n",
    "    print(\"===========\")\n",
    "    print(\"Prompts\")\n",
    "    print(\"===========\")\n",
    "    print(generate_prompts)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"5\") # n number of prompts to generate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
